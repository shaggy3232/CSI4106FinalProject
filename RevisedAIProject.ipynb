{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\n",
    "import matplotlib.pyplot as plot\n",
    "# we can use the LabelEncoder to encode the gender feature\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "# importing two different imputation methods that take into consideration all the features when predicting the missing values\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# oversample the minority class using SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset (1)\n",
    "df = pd.read_csv('./data/ToddlerAutismdatasetJuly2018.csv')\n",
    "# print the dimensionality of the dataframe (1)\n",
    "print(f\"dataframe shape:\\n{df.shape}\\n\")\n",
    "# print the names of the columns that can be used as features when training the machine learning model (1)\n",
    "print(f\"dataframe columns:\\n{df.columns}\\n\")\n",
    "# print the different data types that can be identified from the entire dataset (1)\n",
    "print(f\"dataframe info:\\n{df.info()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate there no missing missing rwos in data\n",
    "missing_rows = df[df.isnull().any(axis=1)]\n",
    "print(missing_rows.size)\n",
    "\n",
    "# load the dataset (1)\n",
    "df = pd.read_csv('./data/ToddlerAutismdatasetJuly2018.csv')\n",
    "# print the dimensionality of the dataframe (1)\n",
    "print(f\"dataframe shape:\\n{df.shape}\\n\")\n",
    "# print the names of the columns that can be used as features when training the machine learning model (1)\n",
    "print(f\"dataframe columns:\\n{df.columns}\\n\")\n",
    "# print the different data types that can be identified from the entire dataset (1)\n",
    "print(f\"dataframe info:\\n{df.info()}\\n\")\n",
    "\n",
    "# plotting the class distrabution for our dataset.\n",
    "x_axis = ['No', 'Yes']\n",
    "y_axis = [len(df.loc[df.Class == 'No']),len(df.loc[df.Class == 'Yes']) ]\n",
    "plot.bar(x_axis, y_axis)\n",
    "plot.show()\n",
    "\n",
    "print(df.loc[df.Class==1, 'Age_Mons'].median())\n",
    "# identify features that represent a notable correlation\n",
    "df_corr = df.drop(columns=['Class','Case_No'])\n",
    "corr = df_corr.corr()\n",
    "for col in corr.columns:\n",
    "    print(f\"correlation with column: {col}\")\n",
    "    print(corr[col].sort_values(ascending=False))\n",
    "    print()\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=10, test_size=0.3)\n",
    "\n",
    "for train_index, test_index in split.split(df, df['Class']):\n",
    "    train_set = df.loc[train_index]\n",
    "    test_set = df.loc[test_index]\n",
    "\n",
    "\n",
    "# print the dimensionality of the test dataset (0.5)\n",
    "print(train_set.shape)\n",
    "# print the dimensionality of the training dataset (0.5)\n",
    "print(test_set.shape)\n",
    "\n",
    "# print the proportional distribution of the classes to identify whether or not the classes are equally(or closer) distributed between the train and test datasets (1 + 1)\n",
    "print(train_set.Class.value_counts()/len(train_set))\n",
    "print(test_set.Class.value_counts()/len(test_set))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate the features and the labels to be used in model development, dropped Case_No and Who completed the test due to irrelevance (2)\n",
    "data = train_set.drop(columns=['Class', 'Case_No','Who completed the test', 'Qchat-10-Score'])\n",
    "labels = train_set['Class'].to_numpy(copy=True)\n",
    "\n",
    "\n",
    "# print the dimensionality of the dataset and the labels (0.5 + 0.5)\n",
    "print(data.shape)\n",
    "print(labels.shape)\n",
    "\n",
    "\n",
    "# select one of the scaling strategies and briefly explain why it is essential to scale your features in the markdown cell mentioned below \n",
    "\n",
    "# create the necessary pipelines and combine the features to be used as the training data for the given algorithm \n",
    "numerical_pipeline = Pipeline([\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "num_feature_names = data.drop(columns=['Sex', 'Ethnicity', 'Jaundice', 'Family_mem_with_ASD']).columns\n",
    "cat_feature_names = ['Sex', 'Ethnicity', 'Jaundice', 'Family_mem_with_ASD']\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "        (\"num\", numerical_pipeline, num_feature_names),\n",
    "        (\"cat\", OneHotEncoder(), cat_feature_names),\n",
    "    ])\n",
    "\n",
    "input_x = full_pipeline.fit_transform(data)\n",
    "\n",
    "\n",
    "questions = [0,1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "question_combinations = [\",\".join(map(str, comb)) for comb in combinations(questions, 3)]\n",
    "training_data_list = []\n",
    "\n",
    "for i, c in enumerate(question_combinations):\n",
    "    question_list = c.split(',')\n",
    "    training_data_list.append([])\n",
    "    for row in input_x:\n",
    "        training_data_list[i].append(np.delete(row, [int(question_list[0]),int(question_list[1]),int(question_list[2])]))\n",
    "\n",
    "print(training_data_list[0:3])\n",
    "print(len(input_x[0]))\n",
    "print(training_data_list[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating different models to train the data set on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_svc = SVC()\n",
    "model_dtree = DecisionTreeClassifier(random_state=42)\n",
    "model_rforest = RandomForestClassifier(random_state=42)\n",
    "model_nb = GaussianNB()\n",
    "model_logreg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scoring = {\"accuracy\": \"accuracy\", \"bal_accuracy\": \"balanced_accuracy\", \"F1_macro\": \"f1_macro\"}\n",
    "\n",
    "parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10], 'gamma':[0.001, 0.0001]}\n",
    "gridSearchCV_svm = GridSearchCV(model_svc, parameters, scoring=scoring, n_jobs = -1, refit='bal_accuracy')\n",
    "\n",
    "parameters = {'max_depth':(3, 4), 'min_samples_split':[5, 10], 'min_samples_leaf':[10, 20]}\n",
    "gridSearchCV_dtc = GridSearchCV(model_dtree, parameters, scoring=scoring, n_jobs = -1, refit='bal_accuracy')\n",
    "\n",
    "parameters = {}  # param_grid={}\n",
    "gridSearchCV_nby = GridSearchCV(model_nb, parameters, scoring=scoring, n_jobs = -1, refit='bal_accuracy')\n",
    "\n",
    "\n",
    "\n",
    "parameters = {'n_estimators':[100, 200], 'max_depth':[3, 5], 'bootstrap':(True, False)}\n",
    "gridSearchCV_rdf = GridSearchCV(model_rforest, parameters, scoring=scoring, n_jobs = -1, refit='bal_accuracy')\n",
    "# fit the training data (0.5)\n",
    "gridSearchCV_dtc_list = []\n",
    "gridSearchCV_rdf_list = []\n",
    "gridSearchCV_svm_list = []\n",
    "gridSearchCV_nby_list = []\n",
    "\n",
    "for i in range(0,5):\n",
    "    scoring = {\"accuracy\": \"accuracy\", \"bal_accuracy\": \"balanced_accuracy\", \"F1_macro\": \"f1_macro\"}\n",
    "\n",
    "    parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10], 'gamma':[0.001, 0.0001]}\n",
    "    gridSearchCV_svm = GridSearchCV(model_svc, parameters, scoring=scoring, n_jobs = -1, refit='bal_accuracy')\n",
    "\n",
    "    parameters = {'max_depth':(3, 4), 'min_samples_split':[5, 10], 'min_samples_leaf':[10, 20]}\n",
    "    gridSearchCV_dtc = GridSearchCV(model_dtree, parameters, scoring=scoring, n_jobs = -1, refit='bal_accuracy')\n",
    "\n",
    "    parameters = {}  # param_grid={}\n",
    "    gridSearchCV_nby = GridSearchCV(model_nb, parameters, scoring=scoring, n_jobs = -1, refit='bal_accuracy')\n",
    "\n",
    "\n",
    "\n",
    "    parameters = {'n_estimators':[100, 200], 'max_depth':[3, 5], 'bootstrap':(True, False)}\n",
    "    gridSearchCV_rdf = GridSearchCV(model_rforest, parameters, scoring=scoring, n_jobs = -1, refit='bal_accuracy')\n",
    "    \n",
    "    gridSearchCV_dtc.fit(training_data_list[i], labels)\n",
    "    gridSearchCV_rdf.fit(training_data_list[i], labels)\n",
    "    gridSearchCV_svm.fit(training_data_list[i], labels)\n",
    "    gridSearchCV_nby.fit(training_data_list[i], labels)\n",
    "\n",
    "    gridSearchCV_dtc_list.append(gridSearchCV_dtc)\n",
    "    gridSearchCV_rdf_list.append(gridSearchCV_rdf)\n",
    "    gridSearchCV_svm_list.append(gridSearchCV_svm)\n",
    "    gridSearchCV_nby_list.append(gridSearchCV_nby)\n",
    "\n",
    "# print the best parameters (0.5)\n",
    "print(gridSearchCV_dtc.best_params_)\n",
    "print(gridSearchCV_rdf.best_params_)\n",
    "print(gridSearchCV_svm.best_params_)\n",
    "print(gridSearchCV_nby.best_params_)\n",
    "\n",
    "# print the best estimator (0.5)\n",
    "print(gridSearchCV_dtc.best_estimator_)\n",
    "print(gridSearchCV_rdf.best_estimator_)\n",
    "print(gridSearchCV_svm.best_estimator_)\n",
    "print(gridSearchCV_nby.best_estimator_)\n",
    "\n",
    "# print the best score from trained GridSearchCV model (0.5)\n",
    "\n",
    "print(gridSearchCV_dtc.best_score_)\n",
    "print(gridSearchCV_rdf.best_score_)\n",
    "print(gridSearchCV_svm.best_score_)\n",
    "print(gridSearchCV_nby.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_value_models = []\n",
    "for i in range(len(gridSearchCV_svm_list)):\n",
    "    best_svm_accuracy = 0\n",
    "    best_svm_accuracy_score = 0\n",
    "    best_svm_balanced = 0\n",
    "    best_svm_balanced_score = 0\n",
    "    best_svm_f1 = 0\n",
    "    best_svm_f1_score = 0\n",
    "    # print the grid search cross-validation results listing the above mentioned evaluation methods (3)\n",
    "    print('For Model' , i ) \n",
    "    print(\"svm cross-validation\")\n",
    "    print(\"max acuracy:\")\n",
    "    for n in range(0,5):\n",
    "        acc = gridSearchCV_svm_list[i].cv_results_[f\"split{n}_test_accuracy\"]\n",
    "        if max(acc) > best_svm_accuracy_score:\n",
    "            best_svm_accuracy_score = max(acc)\n",
    "            best_svm_accuracy = i\n",
    "        print(max(acc))\n",
    "    print(\"max balanced accuracy for svm:\")\n",
    "\n",
    "    for n in range(0,5):\n",
    "        bal_acc = gridSearchCV_svm_list[i].cv_results_[f\"split{n}_test_bal_accuracy\"]\n",
    "        if max(bal_acc)>best_svm_balanced_score:\n",
    "            best_svm_balanced_score = max(bal_acc)\n",
    "            best_svm_balanced = i\n",
    "        print(max(bal_acc))\n",
    "    print(\"max f1_macro svm per \")\n",
    "    for n in range(0,5):\n",
    "        f1m = gridSearchCV_svm_list[i].cv_results_[f\"split{n}_test_F1_macro\"]\n",
    "        if max(f1m) > best_svm_f1_score:\n",
    "            best_svm_f1_score = max(f1m)\n",
    "            best_svm_f1 = i\n",
    "        print(max(f1m))\n",
    "best_value_models.append(best_svm_accuracy)\n",
    "best_value_models.append(best_svm_balanced)\n",
    "best_value_models.append(best_svm_f1)\n",
    "    \n",
    "\n",
    "for i in range(len(gridSearchCV_dtc_list)):\n",
    "    best_dtc_accuracy = 0\n",
    "    best_dtc_accuracy_score = 0\n",
    "    best_dtc_balanced = 0\n",
    "    best_dtc_balanced_score = 0\n",
    "    best_dtc_f1 = 0\n",
    "    best_dtc_f1_score = 0\n",
    "    print('For Model', i)\n",
    "    print(\"dtc cross-validation:\")\n",
    "    print(\"max acuracy dtc\")\n",
    "    for n in range(0,5):\n",
    "        acc = gridSearchCV_dtc_list[i].cv_results_[f\"split{n}_test_accuracy\"]\n",
    "        if max(acc) > best_dtc_accuracy_score:\n",
    "            best_dtc_accuracy_score = max(acc)\n",
    "            best_dtc_accuracy = i\n",
    "        print(max(acc))\n",
    "    print(\"max balanced accuracy:\")\n",
    "\n",
    "    for n in range(0,5):\n",
    "        bal_acc = gridSearchCV_dtc_list[i].cv_results_[f\"split{n}_test_bal_accuracy\"]\n",
    "        if max(bal_acc)>best_dtc_balanced_score:\n",
    "            best_dtc_balanced_score = max(bal_acc)\n",
    "            best_dtc_balanced = i\n",
    "        print(max(bal_acc))\n",
    "    print(\"max f1_macro:\")\n",
    "\n",
    "    for n in range(0,5):\n",
    "        f1m = gridSearchCV_dtc_list[i].cv_results_[f\"split{n}_test_F1_macro\"]\n",
    "        if max(f1m) > best_dtc_f1_score:\n",
    "            best_dtc_f1_score = max(f1m)\n",
    "            best_dtc_f1 = i\n",
    "        print(max(f1m))\n",
    "\n",
    "best_value_models.append(best_dtc_accuracy)\n",
    "best_value_models.append(best_dtc_balanced)\n",
    "best_value_models.append(best_dtc_f1)\n",
    "for i in range(len(gridSearchCV_rdf_list)):\n",
    "    best_rdf_accuracy = 0\n",
    "    best_rdf_accuracy_score = 0\n",
    "    best_rdf_balanced = 0\n",
    "    best_rdf_balanced_score = 0\n",
    "    best_rdf_f1 = 0\n",
    "    best_rdf_f1_score = 0\n",
    "    print('For Model', i)\n",
    "    print(\"rdf cross-validation:\")\n",
    "    print(\"max acuracy:\")\n",
    "\n",
    "    for n in range(0,5):\n",
    "        acc = gridSearchCV_rdf_list[i].cv_results_[f\"split{n}_test_accuracy\"]\n",
    "        if max(acc) > best_rdf_accuracy_score:\n",
    "            best_rdf_accuracy_score = max(acc)\n",
    "            best_rdf_accuracy = i\n",
    "        print(max(acc))\n",
    "    print(\"max balanced accuracy\")\n",
    "\n",
    "    for n in range(0,5):\n",
    "        bal_acc = gridSearchCV_rdf_list[i].cv_results_[f\"split{n}_test_bal_accuracy\"]\n",
    "        if max(bal_acc)>best_rdf_balanced_score:\n",
    "            best_rdf_balanced_score = max(bal_acc)\n",
    "            best_rdf_balanced = i\n",
    "        print(max(bal_acc))\n",
    "    print(\"\\nmax f1_macro\")\n",
    "\n",
    "    for n in range(0,5):\n",
    "        f1m = gridSearchCV_rdf_list[i].cv_results_[f\"split{n}_test_F1_macro\"]\n",
    "        if max(f1m) > best_rdf_f1_score:\n",
    "            best_rdf_f1_score = max(f1m)\n",
    "            best_rdf_f1 = i\n",
    "        print(max(f1m))\n",
    "best_value_models.append(best_rdf_accuracy)\n",
    "best_value_models.append(best_rdf_balanced)\n",
    "best_value_models.append(best_rdf_f1)\n",
    "for i in range(len(gridSearchCV_nby_list)):\n",
    "    best_nby_accuracy = 0\n",
    "    best_nby_accuracy_score = 0\n",
    "    best_nby_balanced = 0\n",
    "    best_nby_balanced_score = 0\n",
    "    best_nby_f1 = 0\n",
    "    best_nby_f1_score = 0\n",
    "    print('For Model', i)\n",
    "    print(\"nby cross-validation results:\")\n",
    "    print(\"max acuracy\")\n",
    "    for n in range(0,5):\n",
    "        acc = gridSearchCV_nby_list[i].cv_results_[f\"split{n}_test_accuracy\"]\n",
    "        if max(acc) > best_nby_accuracy_score:\n",
    "            best_nby_accuracy_score = max(acc)\n",
    "            best_nby_accuracy = i\n",
    "        print(max(acc))\n",
    "    print(\"max balanced accuracy:\")\n",
    "    for n in range(0,5):\n",
    "        bal_acc = gridSearchCV_nby_list[i].cv_results_[f\"split{n}_test_bal_accuracy\"]\n",
    "        if max(bal_acc)>best_nby_balanced_score:\n",
    "            best_nby_balanced_score = max(bal_acc)\n",
    "            best_nby_balanced = i\n",
    "        print(max(bal_acc))\n",
    "    print(\"max f1_macro\")\n",
    "    for n in range(0,5):\n",
    "        f1m = gridSearchCV_nby_list[i].cv_results_[f\"split{n}_test_F1_macro\"]\n",
    "        if max(f1m) > best_nby_f1_score:\n",
    "            best_nby_f1_score = max(f1m)\n",
    "            best_nby_f1 = i\n",
    "        print(max(f1m))\n",
    "best_value_models.append(best_nby_accuracy)\n",
    "best_value_models.append(best_nby_balanced)\n",
    "best_value_models.append(best_nby_f1)\n",
    "print(best_value_models)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a dummy classifier to identify a simple baseline (i.e., a majority class baseline) so that you can compare your prediction results (3)\n",
    "for i in range(len(training_data_list)):\n",
    "    dummy_clf = DummyClassifier(strategy='most_frequent')\n",
    "    dummy_clf.fit(training_data_list[i], labels)\n",
    "    dummy_clf.score(training_data_list[i], labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the test data to be predicted (2)\n",
    "test_data = test_set.drop(columns=['Class', 'Case_No','Who completed the test', 'Qchat-10-Score'])\n",
    "test_labels = test_set['Class'].to_numpy(copy=True)\n",
    "\n",
    "test_data_list = []\n",
    "\n",
    "\n",
    "# print the dimensionality of the dataset and the labels (0.5 + 0.5)\n",
    "print(test_data.shape)\n",
    "print(test_labels.shape)\n",
    "\n",
    "# transform test data for prediction (2)\n",
    "input_x_test = full_pipeline.transform(test_data)\n",
    "\n",
    "for i, c in enumerate(question_combinations):\n",
    "    question_list = c.split(',')\n",
    "    test_data_list.append([])\n",
    "    for row in input_x_test:\n",
    "        test_data_list[i].append(np.delete(row, [int(question_list[0]),int(question_list[1]),int(question_list[2])]))\n",
    "\n",
    "# obtain predictions on test data using the best model from GridSearchCV (i.e., .best_estimator_) (2)\n",
    "test_predictions_svm =  gridSearchCV_svm.best_estimator_.predict(test_data_list[0])\n",
    "\n",
    "# obtain predictions on test data using the best model from GridSearchCV (i.e., .best_estimator_) (2)\n",
    "test_predictions_rdf =  gridSearchCV_rdf.best_estimator_.predict(test_data_list[0])\n",
    "\n",
    "# obtain predictions on test data using the best model from GridSearchCV (i.e., .best_estimator_) (2)\n",
    "test_predictions_nby =  gridSearchCV_nby.best_estimator_.predict(test_data_list[0])\n",
    "\n",
    "# obtain predictions on test data using the best model from GridSearchCV (i.e., .best_estimator_) (2)\n",
    "test_predictions_dtc =  gridSearchCV_dtc.best_estimator_.predict(test_data_list[0])\n",
    "\n",
    "# generate the classification report and the confusion matrix for test predictions (3)\n",
    "cr_test_svm = classification_report(test_labels, test_predictions_svm)\n",
    "cm_test_svm = confusion_matrix(test_labels, test_predictions_svm)\n",
    "\n",
    "print(\"SVM\")\n",
    "print(cr_test_svm)\n",
    "print(cm_test_svm)\n",
    "\n",
    "# generate the classification report and the confusion matrix for test predictions (3)\n",
    "cr_test_rdf = classification_report(test_labels, test_predictions_rdf)\n",
    "cm_test_rdf = confusion_matrix(test_labels, test_predictions_rdf)\n",
    "\n",
    "print('rdf')\n",
    "print(cr_test_rdf)\n",
    "print(cm_test_rdf)\n",
    "\n",
    "# generate the classification report and the confusion matrix for test predictions (3)\n",
    "cr_test_nby = classification_report(test_labels, test_predictions_nby)\n",
    "cm_test_nby = confusion_matrix(test_labels, test_predictions_nby)\n",
    "\n",
    "print('nby')\n",
    "print(cr_test_nby)\n",
    "print(cm_test_nby)\n",
    "\n",
    "# generate the classification report and the confusion matrix for test predictions (3)\n",
    "cr_test_dtc = classification_report(test_labels, test_predictions_dtc)\n",
    "cm_test_dtc = confusion_matrix(test_labels, test_predictions_dtc)\n",
    "\n",
    "print('dtc')\n",
    "print(cr_test_dtc)\n",
    "print(cm_test_dtc)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4dc4c312f2ea8a2889a08f8089a69636e3bfc6ba4c4dfe0a85e7162a7bb5f6f0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('csi4106')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
